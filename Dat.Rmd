---
title: "Project least squares"
author: "File"
date: "12/16/2020"
output:
#  word_document: default
  #html_document: default
   pdf_document: default
---

$\beta_{1}$ = Season

$\beta_{2}$ = year

$\beta_{3}$ = month

$\beta_{4}$ = hour

$\beta_{5}$ = holiday

$\beta_{6}$ = weekday

##Dataset Info:

Attribute Information:

Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv

- instant: record index
- dteday : date
- season : season (1:winter, 2:spring, 3:summer, 4:fall)
- yr : year (0: 2011, 1:2012)
- mnth : month ( 1 to 12)
- hr : hour (0 to 23)
- holiday : weather day is holiday or not (extracted from [Web Link])
- weekday : day of the week
- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
+ weathersit :
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered




#function here is ment to convert hour to days..so we are able to use this dataset instead of picking one over the other.
```{r}
library(tidyverse)
dataset = read.csv("hour.csv") # dataset we are using
convert_hour_to_day <- function(hour){
  require(tidyverse)
  day = hour %>%
    group_by(dteday, season, yr, mnth, holiday, weekday, workingday) %>%
    summarize(weathersit = as.integer(round(mean(weathersit))),
              temp = mean(temp),
              atemp = mean(atemp),
              hum = mean(hum),
              windspeed = mean(windspeed),
              casual = sum(casual),
              registered = sum(registered),
              cnt = sum(cnt))
  #day = tibble::rowid_to_column(day, "instant")
  return(day)
}
```

```{r, echo = FALSE}
summary(dataset)
```

```{r, echo = FALSE}
str(dataset)
```


```{r, echo = FALSE}
table(is.na(dataset)) # no missing values in the date set
```


#Histrogram of the dataset:
```{r, echo = FALSE}

hist(dataset$season)
hist(dataset$hum)
hist(dataset$holiday)
hist(dataset$workingday)
hist(dataset$temp)
hist(dataset$atemp)
hist(dataset$windspeed)
```

```{r}
ggplot(dataset,
       aes(x = mnth, y = cnt, color = season)) + geom_point() +
  labs(x = "month", y = "count", title='Monthly distribution of counts(bikes) in the season')
```

```{r, echo = FALSE}
# create a boxplot for temperature by season
boxplot(temp ~ season, data = dataset, xlab = "Season", ylab = "Temperature", main = "Temperature by Season", col = "skyblue")
```

#PCA: 
```{r, echo = FALSE}

day = data.frame(convert_hour_to_day(dataset))
quant_variables = c('temp','atemp','hum','windspeed','casual',
                    'registered')
# function to fit PCA with default scaling is True
pca_func <- function(dat, scaling =TRUE){
 mod =  prcomp(dat,scale. = scaling)
 print(summary(mod))
 plot(cumsum(mod$sdev^2)/sum(mod$sdev^2),type='o',ylab='percentage of cumulative variance explained',xlab = 'number of components', main = 'PCA Curve')
 return (mod)
}
# fitting to data with all quant variables
PCA = pca_func(day[,quant_variables])
```
Since 3 components explain 82% of the variance, therefore, we can select 3 components and do the KNN with the three component based coordinates and other categorical variables.

#KNN
```{r, echo = FALSE}
quant_data = PCA$x[,1:3]
qual_data = c("season", "yr", "mnth", "holiday" ,"weekday", "workingday", "weathersit")
full_data = cbind(quant_data,day[qual_data])
full_data$cnt = day[,"cnt"]#[['cnt']]
# coverting categorical data to factors
for ( j in qual_data){
  full_data[,j] = factor(full_data[,j])#[[j]])
}
set.seed(100)
# splitting data into training and test set where test size is 30%
test_index = sample(1:nrow(full_data),size = round(0.3*nrow(full_data)))
training_data = full_data[-test_index,]
test_data = full_data[test_index,]

# fitting a KNN on the training set to decide the number of neighbors
library(caret)

# analysis with 3 times repeated cross validation
control <- trainControl(method="repeatedcv",repeats = 3) 
# setting a grid of nearest neigbours from 1 to 20 on which the evaluation will be done
grid <- expand.grid(k=1:20)
# fitting knn
knn_training <- train(cnt ~ ., data = training_data, method = "knn", trControl = control,  tuneGrid = grid)

#plotting the knn model's RMSE
plot(knn_training, main = 'RMSE Vs. KNN')
print(knn_training)

# prediction using the best model
predictions_test <- predict(knn_training,newdata = test_data )
# plotting the predictions against true value
plot(x=test_data$cnt, y=predictions_test, xlab='true count',ylab='predicted count', main = 'predicted vs actual count',cex=0.6)
# getting the R-squared using predicted values
print(paste('The R-squared is ',cor(predictions_test,test_data$cnt)^2))
```













