% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={LeastSquares},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage[width=\textwidth]{caption}
\setcounter{MaxMatrixCols}{20}

\title{University California Davis \\ Least Squares \\ MAT-167 \\ December 19,2020}
% sort your names alphabetically by last name
\author{Prithika Rajendran\\  William Shih
}
\date{\vspace{-2.5em}}
\newpage
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\hypertarget{introduction}{%
\section{1. Introduction}\label{introduction}}
Bike sharing rental systems has become increasingly popular throughout the years. Especially with the increasing amounts of bike rental programs that are being offered for people to look for different means of transportation. As these bike rental programs increased so did the the interest in learning about their important role in traffic, environmental and health issues. The goal of this data set it to do a prediction of bike rental count hourly based on the environmental and seasonal settings.

\hypertarget{Background}{%
\section{2. Background}\label{Background}}
The \color{blue}
\href{https://www.archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset}{dataset}
\color{black} for this project is the ``Bike Sharing Dataset Data Set''
found in the UCI Machine Learning Repository. The dataset contains
hourly count of rental bikes for all of 2011 and 2012 (January 1, 2011
to December 31, 2012) in the Capital Bikeshare System of Washington D.C.
area (Washington-Arlington-Alexandria, DC-VA-MD-WV metropolitan area).
The UCI Machine Learning Repository cites Hadi Fanaee-T from the
``Laboratory of Artificial Intelligence and Decision Support (LIAAD),
University of Porto'' for the compilation of the data.

The dataset is outdated since data is actually available up to November
2020 on Capital Bikeshare's website (as of December 18, 2020), but this
limited dataset will still work for the purposes of demonstrating linear
algebra on a real world dataset.

There are two files included in the dataset: a \texttt{hour.csv} and a
\texttt{day.csv}. We will use the \texttt{hour.csv} for the regression,
since the \texttt{day.csv} is simply just a sumamry of the
\texttt{hour.csv} file. We also made a function that easily converts the
\texttt{hour.csv} to the \texttt{day.csv} called
\texttt{convert\_hour\_to\_day()}.

There are 14 different variables that are in this dataset that are potentially of interest. Two variables are not useful and immediately thrown out: \texttt{instant} (this is simply the row number of the dataset) and \texttt{dteday} (date of the year).

\hypertarget{Dataset}{%
\subsection{2.1 Dataset}\label{Dataset}}
Denote \(x_{n}\) as plausible independent variables and denote \(y_{n}\)
as plausible dependent variables.

\(x_{1}\): \texttt{season} (1: spring, 2: summer, 3: fall, 4: winter)

\(x_{2}\): \texttt{yr} (0: 2011, 1: 2012)

\(x_{3}\): \texttt{mnth} (1 to 12)

\(x_{4}\): \texttt{hour} (0 to 23)

\(x_{5}\): \texttt{holiday} (whether a holiday (0 or 1) from
\color{blue}
\href{https://dchrc.dc.gov/page/holiday-schedule}{this list of holidays}
\color{black} )

\(x_{6}\): \texttt{weekday} (0 to 6)

\(x_{7}\): \texttt{workingday} (1 if weekday and not holiday, 0
otherwise)

\(x_{8}\): \texttt{weathersit}: Weather conditions (1: Clear, Few
clouds, Partly cloudy, Partly cloudy, 2: Mist + Cloudy, Mist + Broken
clouds, Mist + Few clouds, Mist, 3: Light Snow, Light Rain +
Thunderstorm + Scattered clouds, Light Rain + Scattered clouds, 4: Heavy
Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog)

\(x_{9}\): \texttt{temp} (0-1, normalized temperature in Celsius.
Divided by 41)

\(x_{10}\): \texttt{atemp} (0-1, normalized ``feels like'' temperature
in Celsius. Divided by 50)

\(x_{11}\): \texttt{hum} (percent humidity)

\(x_{12}\): \texttt{windspeed} (0-1, Normalized wind speed. Divided by
67)

\(y_{1}\): \texttt{casual} (count of casual users)

\(y_{2}\): \texttt{registered} (count of registered users)

\(y_{3}\): \texttt{cnt} (count of sum of casual and registered users)

The following least squares regression exercise will try to predict the
\texttt{casual}, \texttt{registered}, or \texttt{cnt} as a function of
the independent variables. Also, we will try some principal components
analysis (PCA) and k-nearest neighbors (kNN) with these variables.

\hypertarget{data-analysis}{%
\subsection{2.2 Data Analysis}\label{data-analysis}}

Preliminary data analysis shows that the \texttt{hour} is by far the most important independent variable for explaining the variation in the dependent variables. Thus, it is important to know how exactly the \texttt{hour} variable interacts with \texttt{registered}, \texttt{casual}, and \texttt{cnt}.

We also found that it makes sense to treat \texttt{hour} as a
categorical variable (treat \texttt{hour} as 23 independent dummy
variables (0 or 1 for each variable), one for each hour minus the
constant term), but in this case, we will try to fit \texttt{hour} in terms of a polynomial curve to demonstrate polynomial fitting with linear algebra.

A plot of \texttt{hour} on the x-axis and \texttt{registered} or
\texttt{casual} on the y-axis would us some insight of what degree polynomial for \texttt{hour} we should be looking for.

\begin{figure}
\centering
\includegraphics{pic1.png}
\caption{Boxplot of registered users for Jan 1, 2011 to December 31, 2011 for each hour of day. The low whisker represents 10th percentile, the box represents the interquartile range, the top whisker represents the 90th percentile. For example, with 731 days in the dataset, the low whisker represents the 73rd lowest value.}
\end{figure}

\textbf{Figure 1} for the number of registered users show a trimodal distribution with three peaks throughout the day. A possible interpretation of the three peaks is that there is an early peak for the morning commute, a central peak for lunchtime, and a late peak for the evening commute. This suggests that we need a high-degree polynomial to accurately the number of registered users throughout the day. A six-degree polynomial is the minimum degree that can represent a trimodal distribution.

\begin{figure}
\centering
\includegraphics{pic2.png}
\caption{Boxplot of casual users for Jan 1, 2011 to December 31, 2011 for each hour of day. The low whisker represents 10th percentile, the box represents the interquartile range, the top whisker represents the 90th percentile. For example, with 731 days in the dataset, the low whisker represents the 73rd lowest value.}
\end{figure}
\newpage
\textbf{Figure 2} for the number of casual users show a single peak distribution. A possible reason for this is that casual users are tourists that don't commute. Tourists also don't wake up early in the morning and most things to do for tourists occur in the afternoon and evening. Thus, the peak occurs at around 12 PM-6 PM. A two-degree polynomial is potentially sufficient to represent the number of casual users throughout the day.

\textbf{Figure 1} and \textbf{Figure 2} show significant variation so that it is clear that the hour of the day is not the only variable influencing how many users there are for this bike sharing system. We can plot at the number of users for each day for further information.
\newpage
\begin{figure}
\centering
\includegraphics{pic3.png}
\caption{Number of casual, registered, and sum of casual and registered for all 731 days in the dataset}
\end{figure}

\textbf{Figure 3} Shows both variation through the year and an increase in number of users from 2011 to 2012. This makes sense if this Capital Bike-Sharing was still a developing system in 2011 and not yet a mature system where the market is already saturated. Thus, we want to include a variable in our least squares regression model that includes controls for seasonal variation and the year. This would be \texttt{season} and
\texttt{yr} from the list of \(x_{n}\). It turns out that the
\texttt{weekday} (day of the week) and \texttt{mnth} (month of the year) do not explain a large additional amount of variation, so they won't be included in the model.

But \textbf{Figure 3} still shows quite a bit of variation day to day within each season. There are quite a few weather related variables in the list of independent variables in the dataset, which would account for some of the remaining variation.

\newpage

\begin{figure}
\centering
\includegraphics{pic4.png}
\caption{Normalized temperature and ``feels like'' temperature for all 731 days in the dataset}
\end{figure}

\textbf{Figure 4} shows the average \texttt{temp} (actual temperature) and \texttt{atemp} (``feels like'' temperature) for each day of the year. A comparison of \textbf{Figure 3} and \textbf{Figure 4} shows that temperature shows a strong negative relationship with the number of bike-sharing users, which makes sense. People do not want to bike when it is cold outside. These variables have a very high correlation with each other such that we found it to be sufficient to just add \texttt{temp}. In fact, \texttt{temp} is good enough to explain most of the weather-related variation and other variables such as \texttt{wind} (wind speed) and \texttt{hum} (humidity) are not necessary.

Thus, we choose just \texttt{season}, \texttt{yr}, \texttt{hr}, and \texttt{temp} as the independent variables and omit the rest of the variables for the least squares regression model. These are \(x_{1}, x_{2}, x_{4}\), and \(x_{9}\).

\newpage

\hypertarget{design-matrix}{%
\section{3. Design Matrix}\label{design-matrix}}

The design matrix \(A\) is the matrix that will be used to solve the equation \(Ax = b\), where \(x\) are the coefficients, often referred to as the \(\beta\)'s and \(b\) is the dependent variable. It is the matrix of the explanatory/independent variables.

The design matrix \(A\) will be of \(\mathbb{R}^{17379 \times (n+4)}\), (where \(n\) is the maximum degree of the polynomial for \(x_{4}\) (\texttt{hour})) since there are 17,379 rows (one for each hour in the dataset) and there are four variables other than the \texttt{hour} (\(x_{4}\)) variable. The other four variables are the constant term, \(x_{1}\) (\texttt{season}), \(x_{2}\) (\texttt{yr}), and \(x_{9}\) (\texttt{temp}).

The following will be the form of our design matrix
\(A \in \mathbb{R}^{17379 \times (n+4)}\) (Note that \(m\) is left the matrix for simplification, but the of \(m = 17379\)):

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & \dots & x_{14}^{n} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & \dots & x_{24}^{n} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & \dots & x_{34}^{n} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & \dots & x_{m4}^{n} & x_{m9} \\
  \end{bmatrix} 
\]

The first value of the index represents the row number and the second value of the index represents the variable number in the matrix. For example \(x_{32}\) in the above means the 3rd row of variable \(x_{2}\).

Now the question remaining is the appropriate value of \(n\). We know that the answer depends on which dependent variable we are using. If \(b\) is \texttt{registered} or \texttt{cnt}, the value of \(n\) should be higher than if \(b\) is \texttt{casual}. One way to find a good for \(n\) is calculate the R-squared for each value of \(n\). The R-squared is the proportion of variation in the dependent variable that can be explained by the independent variables. A value of \(n\) where \(n+1\) does not increase the R-squared significantly further would be a good value of \(n\).

\begin{longtable}[]{@{}lrrr@{}}
\caption{R-squared values for linear regression of each of value of \(n\), maximum power of polynomial for \texttt{hour}, including \(x_{1}, x_{2}\), and \(x_{9}\)}\tabularnewline
\toprule
& \(R^{2}_{cnt}\) & \(R^{2}_{casual}\) &
\(R^{2}_{registered}\)\tabularnewline
\midrule
\endfirsthead
\toprule
& \(R^{2}_{cnt}\) & \(R^{2}_{casual}\) &
\(R^{2}_{registered}\)\tabularnewline
\midrule
\endhead
\(x_{4}\) & 0.343 & 0.285 & 0.291\tabularnewline
\(x_{4}+x_{4}^{2}\) & 0.464 & 0.371 & 0.394\tabularnewline
\(x_{4}+x_{4}^{2}+x_{4}^{3}\) & 0.515 & 0.430 & 0.431\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{4}\) & 0.516 & 0.440 &
0.436\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{5}\) & 0.530 & 0.446 &
0.464\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{6}\) & 0.552 & 0.446 &
0.497\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{7}\) & 0.589 & 0.446 &
0.551\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{8}\) & 0.599 & 0.446 &
0.565\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{9}\) & 0.606 & 0.447 &
0.575\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{10}\) & 0.606 & 0.447 &
0.575\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{11}\) & 0.613 & 0.447 &
0.585\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{12}\) & 0.614 & 0.447 &
0.585\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{13}\) & 0.638 & 0.448 &
0.619\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{14}\) & 0.638 & 0.448 &
0.619\tabularnewline
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{15}\) & 0.638 & 0.448 &
0.619\tabularnewline
\bottomrule
\end{longtable}

\textbf{Table 1} shows the values of R-squared regressed upon the
dependent variables \texttt{cnt}, \texttt{casual}, and
\texttt{registered} for each value of \(n\) for values of 1 to 15. For \texttt{cnt} and \texttt{registered}, a good value of \(n\) appears to be 7. \(n = 7\) gives a huge increase in R-squared over \(n = 6\) for \texttt{cnt} and \texttt{registered}. Only small increases of R-squared
are seen for values of \(n\) above 7 for \texttt{cnt} and
\texttt{registered}. Although these increases are still statistically significant, it is not a good idea to have very high order polynomials as such a regression is likely to overfit the data. For example, very high order polynomials may give poor predictions for data in 2013 or beyond. For \texttt{casual}, a good value of \(n\) appears to be 3. Only small increases of R-squared are seen for values of \(n\) above 3 for
\texttt{casual}.

Therefore, the design matrix if \(b\) is \texttt{cnt} or
\texttt{registered} is \(A \in \mathbb{R}^{17379 \times 11}\), where
\(m = 17379\):

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & x_{14}^{3} & x_{14}^{4} & x_{14}^{5} & x_{14}^{6} & x_{14}^{7} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & x_{24}^{3} & x_{24}^{4} & x_{24}^{5} & x_{24}^{6} & x_{24}^{7} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & x_{34}^{3} & x_{34}^{4} & x_{34}^{5} & x_{34}^{6} & x_{34}^{7} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & x_{m4}^{3} & x_{m4}^{4} & x_{m4}^{5} & x_{m4}^{6} & x_{m4}^{7} & x_{m9} \\
  \end{bmatrix} 
\]

The design matrix if \(b\) is \texttt{casual} is
\(A \in \mathbb{R}^{17379 \times 7}\), where \(m = 17379\):

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & x_{14}^{3} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & x_{24}^{3} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & x_{34}^{3} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots  \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & x_{m4}^{3} & x_{m9} \\
  \end{bmatrix} 
\]

\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{Comparison of speed (in milliseconds) of custom
design\_matrix() functions with built-in R model.matrix.lm()
function}\tabularnewline
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endfirsthead
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endhead
design\_matrix() & 19.032 & 20.041 & 30.781 & 21.834 & 29.155 & 212.279
& 100\tabularnewline
design\_matrix\_Cpp() & 7.474 & 7.972 & 12.173 & 8.578 & 9.948 & 193.108
& 100\tabularnewline
model.matrix.lm() & 19.072 & 20.411 & 28.093 & 22.914 & 30.381 & 93.841
& 100\tabularnewline
\bottomrule
\end{longtable}

We made a custom function called \texttt{design\_matrix()} in R and a Rcpp (C++) version called \texttt{design\_matrix\_Cpp()} to form the design matrix. \color{blue}
\href{https://teuder.github.io/rcpp4everyone_en/index.html}{This Rcpp tutorial} 
\color{black} was useful in converting the R functions to C++. The R implementation was slightly faster than the base built-in R version called \texttt{model.matrix.lm()} and the C++ version was three times faster than built-in R version. \textbf{Table 2} shows a speed comparison using R package \texttt{microbenchmark} showing minimum, 25th percentile, 50th percentile, mean, 75th percentile, and max time of 100 replications of the function to form the design matrix with \(n\) = 7 and \(A \in \mathbb{R}^{17379 \times 11}\).

\hypertarget{normal-equation}{%
\section{4. Normal Equation}\label{normal-equation}}

The simplest way to solve \(Ax = b\) would be to do use the normal
equations.

The solution to \(Ax = b\) using the normal equations is
\(x = (A^{T}A)^{-1}A^{T}b\)

However, computers cannot represent real numbers exactly. The condition
number \(\kappa(A)\) of the input matrix \(A\) represents how much error
there could be in the output. The condition number of \(A^{T}A\) is
equal to condition number of \(A^{2}\). For our example, as \(n\)
increases (a more complex and higher order polynomial), the condition
number \(\kappa(A)\) increases significantly. If the condition number is
too high, then a computer will treat a non-singular matrix as singular.
Then, there will be very large errors in the computation.

\begin{longtable}[]{@{}llll@{}}
\caption{Condition number of each value of \(n\) (maximum power of
polynomial for \texttt{hour}) and relative error of normal equations
versus SVD}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(\kappa(A)^{2}\)\strut
\end{minipage} & \begin{minipage}[b]{0.36\columnwidth}\raggedright
Relative error/error message R\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Relative error C++\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(\kappa(A)^{2}\)\strut
\end{minipage} & \begin{minipage}[b]{0.36\columnwidth}\raggedright
Relative error/error message R\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
Relative error C++\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(6.49 \times 10^{3}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
\(5.87 \times 10^{-12}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(6.17 \times 10^{-12}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.09 \times 10^{6}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
\(-1.1 \times 10^{-14}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(-1.7 \times 10^{-14}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+x_{4}^{3}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(9.08 \times 10^{8}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
\(1.18 \times 10^{-12}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.41 \times 10^{-12}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{4}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(4.17 \times 10^{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
\(2.87 \times 10^{-12}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.71 \times 10^{-12}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{5}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.03 \times 10^{14}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
\(1.52 \times 10^{-8}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.50 \times 10^{-8}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{6}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.29 \times 10^{17}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(5.89 \times 10^{-18}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.24 \times 10^{-9}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{7}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.12 \times 10^{20}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(6.14 \times 10^{-21}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.18 \times 10^{-3}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{8}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.11 \times 10^{23}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(6.35 \times 10^{-24}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(5.99 \times 10^{-3}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{9}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.22 \times 10^{26}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(6.78 \times 10^{-27}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.06 \times 10^{-1}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.46 \times 10^{29}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(1.29 \times 10^{-29}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.02 \times 10^{1}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.94 \times 10^{32}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(1.07 \times 10^{-32}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.06 \times 10^{1}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{12}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.89 \times 10^{35}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(7.16 \times 10^{-35}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.03 \times 10^{0}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{13}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(4.86 \times 10^{38}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(1.13 \times 10^{-37}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.12 \times 10^{0}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{14}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(3.43 \times 10^{45}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(2.67 \times 10^{-40}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.00 \times 10^{0}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\(x_{4}+x_{4}^{2}+\dots+x_{4}^{15}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(2.04 \times 10^{45}\)\strut
\end{minipage} & \begin{minipage}[t]{0.36\columnwidth}\raggedright
Error, Recripocal \(\kappa(A)\): \(2.51 \times 10^{-43}\)\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
\(1.09 \times 10^{0}\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Table 3} shows the square of the condition number for matrix
\(A\), the relative error/error message when solving \(Ax = b\) for each
value of \(n\) using R, and the relative error using Rcpp (C++). The
relative error is calculated as the maximum error of any coefficient of
the normal equations when compared to SVD (SVD is considered
computationally precise).

However, R gives an error trying to find the inverse if the matrix is
close to singular. The error message is the following: `Error in
solve.default(t(A) \%*\% A): system is computationally singular:
reciprocal condition number ='. The reciprocal condition number given in
the error message by R is close to the conditional number of \(A^{2}\).
On the other hand, RcppArmadillo does not give an error when finding the
inverse of a system with a very high condition number. When \(n > 9\)
and \(\kappa(A^{T}A) > 10^{29}\), the relative error is 100\% or higher.
R appears to give an error if the relative error is greater than 0.1\%
and \(\kappa(A^{T}A) > 10^{17}\). With such large errors possible, it is
not recommended to ever use the normal equations when solving \(Ax = b\)
on a computer.

\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{Comparison of speed (in milliseconds) of solving \(Ax = b\)
where \(A \in \mathbb{R}^{17379 \times 7}\) using normal equations
implemented in R vs Rcpp (C++)}\tabularnewline
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endfirsthead
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endhead
normal\_equations(A, b) & 5.954 & 6.286 & 7.70 & 6.737 & 7.517 & 22.960
& 100\tabularnewline
normal\_equations\_Cpp(A, b) & 2.216 & 2.342 & 2.55 & 2.469 & 2.744 &
4.525 & 100\tabularnewline
\bottomrule
\end{longtable}

\textbf{Table 4} shows that the Rcpp (C++) implementation is on average
4 times faster than the R implementation of solving the normal
equations.

\hypertarget{qr-decomposition}{%
\section{5. QR Decomposition}\label{qr-decomposition}}

Another way, a more computationally accurate way, to solve \(Ax = b\) to
use QR decomposition.

First, find the reduced QR decomposition such that
\(A = \hat{Q}\hat{R}\), \(\hat{Q}\) is an orthogonal matrix, and
\(\hat{R}\) is an upper triangular matrix. \(\hat{Q}\) meets the
following condition: \(\hat{Q}^{T}\hat{Q} = \hat{Q}\hat{Q}^{T} = I\).
Also, \(\hat{Q} \in \mathbb{R}^{17379 \times 7}\) and
\(\hat{R} \in \mathbb{R}^{7 \times 7}\) in this case as
\(A \in \mathbb{R}^{17379 \times 7}\)

The solution to \(Ax = b\) using QR decomposition is
\(x = \hat{R}^{-1}\hat{Q}^{T}b\).

\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{Comparison of speed (in milliseconds) of solving \(Ax = b\)
where \(A \in \mathbb{R}^{17379 \times 7}\) using QR decomposition
implemented in R vs Rcpp (C++)}\tabularnewline
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endfirsthead
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endhead
qr.solve(A, b) & 5.250 & 5.490 & 6.873 & 5.78 & 6.409 & 16.309 &
100\tabularnewline
qr\_solve\_Cpp(A, b) & 5.313 & 5.499 & 6.077 & 5.71 & 6.110 & 15.186 &
100\tabularnewline
\bottomrule
\end{longtable}

\textbf{Table 5} shows that the Rcpp (C++) implementation is slightly
faster than R implementation of using QR decomposition to solve
\(Ax = b\).

\hypertarget{singular-value-decomposition}{%
\section{6. Singular Value
Decomposition}\label{singular-value-decomposition}}

A longer way to solve \(Ax = b\), but an even more accurate way than QR
decomposition is to use Singular Value Decomposition.

First, find the reduced SVD such that \(A = \hat{U}\hat{\Sigma}V^{T}\).

Also, \(\hat{U} \in \mathbb{R}^{17379 \times 7}\),
\(V \in \mathbb{R}^{7 \times 7}\), and
\(\hat{\Sigma} \in \mathbb{R}^{7 \times 7}\) in this case as
\(A \in \mathbb{R}^{17379 \times 7}\).

The solution to \(Ax = b\) using SVD is
\(x = V(\hat{U}^{T}b/\hat{\Sigma})\)

\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{Comparison of speed (in milliseconds) of solving \(Ax = b\)
where \(A \in \mathbb{R}^{17379 \times 7}\) using SVD implemented in R
vs Rcpp (C++)}\tabularnewline
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endfirsthead
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endhead
svd\_solve(A, b) & 9.382 & 9.775 & 11.454 & 10.131 & 10.905 & 26.732 &
100\tabularnewline
svd\_solve\_Cpp(A, b) & 8.117 & 8.324 & 8.778 & 8.619 & 9.017 & 11.376 &
100\tabularnewline
\bottomrule
\end{longtable}

\textbf{Table 6} shows that the Rcpp (C++) implementation is slightly faster than R implementation of using SVD to solve \(Ax = b\).Now, it is time to compare the speed of solving \(Ax = b\). If theory is correct, the normal equations should be fastest, followed by QR decomposition, and followed by SVD.

\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{Comparison of speed (in milliseconds) of solving \(Ax = b\)
where \(A \in \mathbb{R}^{17379 \times 11}\) using QR decomposition or
SVD implemented in R vs Rcpp (C++)}\tabularnewline
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endfirsthead
\toprule
expr & min & lq & mean & median & uq & max & neval\tabularnewline
\midrule
\endhead
normal\_equations\_Cpp(A, b) & 3.838 & 3.980 & 4.201 & 4.058 & 4.429 &
5.403 & 100\tabularnewline
qr.solve(A, b) & 9.968 & 10.575 & 14.597 & 11.218 & 13.927 & 168.261 &
100\tabularnewline
qr\_solve\_Cpp(A, b) & 11.427 & 11.920 & 12.717 & 12.292 & 13.018 &
21.020 & 100\tabularnewline
svd\_solve(A, b) & 19.954 & 20.799 & 23.806 & 22.417 & 27.291 & 34.312 &
100\tabularnewline
svd\_solve\_Cpp(A,b) & 17.413 & 17.957 & 18.833 & 18.477 & 19.313 &
26.481 & 100\tabularnewline
\bottomrule
\end{longtable}

Note that \textbf{Table 7} uses \(n=7\), the design matrix for
\texttt{registered} and \texttt{cnt} instead of \(n=3\), the design
matrix for \texttt{casual} in \textbf{Table 4, 5, 6}.

Normal equations using Rcpp (C++) is about 3 times faster than QR
decomposition using C++. QR decomposition is about 70\% faster than SVD.
QR decomposition implemented in R is about twice as fast as using SVD
implemented in R. Thus, we see that the results from \textbf{Table 7}
are in line with what we should expect from theory.

\hypertarget{final-regression-model}{%
\section{7. Final Regression Model}\label{final-regression-model}}

Recall that \(y_{1}\) = \texttt{casual}, \(y_{3}\) = \texttt{cnt},
\(x_{1}\) = \texttt{season}, \(x_{2}\) = \texttt{yr}, \(x_{4}\) =
\texttt{hour}, \(x_{9}\) = \texttt{temp} for below.

\(y_{1} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{4} + \beta_{4}x_{4}^{2} + \beta_{5}x_{4}^{3} + \beta_{6}x_{9}\)

\(y_{3} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{4} + \beta_{4}x_{4}^{2} + \beta_{5}x_{4}^{3} + \beta_{6}x_{4}^{4} + \beta_{7}x_{4}^{5} + \beta_{8}x_{4}^{6} + \beta_{9}x_{4}^{7} + \beta_{10}x_{9}\)

The solution to \(Ax = b\) for \(b = y_{1}\) is:

\[
x =
  \begin{bmatrix}
    \beta_{0} \\
     \beta_{1} \\
    \beta_{2} \\
     \beta_{3}\\
     \beta_{4} \\
    \beta_{5} \\
    \beta_{6} \\
  \end{bmatrix} =
  \begin{bmatrix}
    -42.145 \\
    0.6875 \\
    12.887 \\
    -4.859 \\
    1.275 \\
    -0.047 \\
    89.528 \\
  \end{bmatrix}
\]

The solution to \(Ax = b\) for \(b = y_{3}\) is:

\[
x =
  \begin{bmatrix}
    \beta_{0} \\
     \beta_{1} \\
    \beta_{2} \\
     \beta_{3}\\
     \beta_{4} \\
    \beta_{5} \\
    \beta_{6} \\
    \beta_{7}\\
     \beta_{8} \\
    \beta_{9} \\
    \beta_{10} \\
  \end{bmatrix} =
  \begin{bmatrix}
    -145.05 \\
    17.18 \\
    88.57 \\
    29.45 \\
    -63.199 \\
    24.27 \\
    -3.62 \\
    0.258 \\
    -0.0088 \\
    0.000116 \\
    243.131 \\
  \end{bmatrix} 
\]

Inputting the solution results in:

\(y_{1} = -42.145 + 0.6875x_{1} + 12.887x_{2} - 4.859x_{4} + 1.275x_{4}^{2} - 0.047x_{4}^{3} + 89.528x_{9}\)

\(y_{3} = -145.05 + 17.18x_{1} + 88.57x_{2} + 29.45x_{4} - 63.199x_{4}^{2} + 24.27x_{4}^{3} - 3.62x_{4}^{4} + 0.258x_{4}^{5} - 0.0088x_{4}^{6} + 0.000116x_{4}^{7} + 243.131x_{9}\)

Note that
\(\frac{1}{n}\sum_{i=1}^{n} x_{1} = \bar{x_{1}} = 2.50164, \frac{1}{n}\sum_{i=1}^{n} x_{2} = \bar{x_{2}} = 0.5025606, \frac{1}{n}\sum_{i=1}^{n} x_{8} = \bar{x_{8}} = 0.4970\).

Input the means of \(x_{1}, x_{2}\), and \(x_{8}\) into the formulas of
\(y_{1}\) and \(y_{3}\).

This results in eliminating all independent variables that aren't
\texttt{hour} (\(x_{4}\)), resulting in the following two functions:

\(y_{1} = -4.859x_{4} + 1.275x_{4}^{2} -0.047x_{4}^{3} + 10.54523\)

\(y_{3} = 29.45x_{4} - 63.199x_{4}^{2} + 24.27x_{4}^{3} - 3.62x_{4}^{4} + 0.258x_{4}^{5} - 0.0088x_{4}^{6} + 0.000116x_{4}^{7} + 63.26103\).

This now allows us to graph \texttt{casual} (\(y_{1}\)) and
\texttt{registered} (\(y_{3}\)) as a function of \(x_{4}\)
(\texttt{hour}) and see how well the least squares regression fits the
actual data.

\newpage

\begin{figure}
\centering
\includegraphics{pic5.png}
\caption{The 25th, 50th, and 75th percentile of casual users (731 days)
are graphed for each hour of the day. The mean values of season, yr, and
temp are entered into the regression solution to give a constant value.
Thus, the regression is now only in terms of hour. Then, the mean curve
fit of the regression can be compared to the 25th, 50th, and 75th
percentiles of casual users.}
\end{figure}

\textbf{Figure 5} shows that the cubic polynomial function is not a great fit for hours 4 to 6 as the mean curve fit is above the 75th percentile. The curve does not dip down far enough from hours 4 to 6. Note that interquartile range (75th percentile minus 25th percentile) for \texttt{casual} is very large from \textbf{Figure 5} which might make predicting data quite difficult. Also \textbf{Figure 5} suggests that \texttt{casual} is skewed right since the mean curve fit is almost always above the 50th percentile.

\newpage

\begin{figure}
\centering
\includegraphics{pic6.png}
\caption{Same description as for Figure 5, except the variable is now cnt (sum of registered and casual users) rather than casual.}
\end{figure}

\textbf{Figure 6} shows the 7-degree (septic) polynomial function is quite a good fit for \texttt{cnt}. However, near hour 22, the mean curve fit starts to slope upwards again, which is not correct. Also, similar to \textbf{Figure 5}, the mean curve fit overestimates the number of users from hours 4 to 6 again.

\hypertarget{principle component analysis}{%
\section{8. Principal Component Analysis }\label{principle componenet analysis}}
Using Principal Component Analysis (PCA) to solve Ax = b. To do principal components we have X data which are uncorrelated projections Y of the data points $X1,
..., Xn$ with each $Xi$ are a set real numbers as a row vector. These X are projected onto some directions unit vectors such that var(Y) is maximal. For the computing of finding variance, standard deviation, covariance for PCA we can uses these formulas below: 

To compute the mean for our x's we use the mean formula: $\bar{X}$ = \( \frac{1}{n} \sum_{i=i}^{n} x_{i} \) \\ 

The find the variance we use: $var(x)$ = \(\frac{1}{n-1} \sum_{i=i}^{n} (x_{i} - \bar{x})^2\)

Also our covariance is cov(x, y) = \(\frac{1}{n-1} \sum_{i=i}^{n} (x_{i} - \bar{x})(y_{i} - \bar{y})\)


\begin{longtable}[]{@{}lrrrrrrr@{}}
\caption{This table gives the standard deviation, proportion of variance explained by each of the principal component, and the cumulative proportion of variance explained.}\tabularnewline
\toprule
Importance of components:\tabularnewline
&PC1 & PC2 &   PC3 &   PC4 &    PC5  &   PC6\tabularnewline
\midrule
\endfirsthead
\toprule
Standard deviation & 1.7015 & 1.1025 & 0.9219 & 0.7758 & 0.65553 & 0.08902\tabularnewline
Proportion of Variance & 0.4825 & 0.2026 & 0.1416 & 0.1003 & 0.07162 & 0.00132\tabularnewline
Cumulative Proportion & 0.4825 & 0.6851 & 0.8267 & 0.9271 & 0.99868 & 1.00000\tabularnewline
\bottomrule
\end{longtable}
\newpage

\begin{figure}
\centering
\includegraphics{PCA_Curve.png}
\caption{This figure showcases how percentage of the variance in the data explained as we add principal components. For example, out PCA1 48 percent of the data set. The PCA2 explains 20 percent and PCA3 explains 14 percent of the data. The total of the 3 components togther 82 percent of the data.}
\end{figure}

We made a custom function called \texttt{pca\_func()} in R which is helpful for capture the majority of variance in your predictors.\color{black} was useful for fitting the PCA, and using the built in function \texttt{prcomp()}. With this function we can prcomp it will performs the principal components analysis on the dataset matrix by taking the SVD.


\hypertarget{k-Nearest Neighbors}{%
\section{9. K-Nearest Neighbors }\label{k-Nearest Neighbors}}
K-nearest neighbour (KNN) is a machine learning algorithm. This algorithms can be used for both classification and regression problems. In addition, (KNN) is supervised learning which means that we know the type of data our data is and what outcomes we are looking for.We are doing a prediction on how "cnt"(count) being our outcome variable affecting predictor variables which our our other variables in our data set. Furthermore, for implementation of the KNN algorithm we will reference back to the PCA problem and use the 3 components based coordinates and other categorical variables which explain the 82 percent variance to do KNN. 

When using the KNN formula we are able to find the distance between two points in featured space we can easy use the Euclidean distance formula to do so:

$d(p,q)$= \sqrt {\sum _{i=1}^{n}  \left( q_{i}-p_{i}\right)^2 }

\newpage

\begin{longtable} {l@\quad|@\quad l@\quad|@\quad l@\quad|@\quad l}
\toprule
k-Nearest Neighbors \tabularnewline
512 samples\tabularnewline
10 predictor\tabularnewline
No pre-processing
Resampling: Cross-Validated (10 fold, repeated 3 times)\tabularnewline 
Summary of sample sizes: 460, 461, 460, 461, 461, 461, ... \tabularnewline
Resampling results across tuning parameters:\tabularnewline
\midrule
 k &  RMSE  &    Rsquared  & MAE\tabularnewline
\toprule
   1 & 925.1560 & 0.7839712 & 690.8140\tabularnewline
   2 & 727.9760 & 0.8608545 & 560.1111\tabularnewline
   3 & 693.0851 & 0.8758349 & 538.5740\tabularnewline
   4 & 670.6142 & 0.8858940 & 526.6748\tabularnewline
   5 & 661.4997 & 0.8910960 & 523.4297\tabularnewline
   6 & 656.4150 & 0.8942347 & 519.3416\tabularnewline
   7 & 656.6373 & 0.8953553 & 522.3512\tabularnewline
   8 & 658.6736 & 0.8959119 & 522.6202\tabularnewline
   9 & 655.1282 & 0.8980874 & 521.0988\tabularnewline
  10 & 657.1985 & 0.8981599 & 524.6721\tabularnewline
  11 & 660.3379 & 0.8978063 & 527.6320\tabularnewline
  12 & 665.1246 & 0.8967227 & 534.0605\tabularnewline
  13 & 668.0968 & 0.8965219 & 536.8265\tabularnewline
  14 & 669.2021 & 0.8964224 & 539.2323\tabularnewline
  15 & 674.8979 & 0.8959398 & 543.9940\tabularnewline
  16 & 675.1185 & 0.8968547 & 546.7142\tabularnewline
  17 & 679.8648 & 0.8962774 & 552.8564\tabularnewline
  18 & 684.0043 & 0.8959587 & 556.5736\tabularnewline
  19 & 694.5197 & 0.8933500 & 565.4170\tabularnewline
  20 & 699.3458 & 0.8931800 & 570.3835\tabularnewline
\bottomrule
\caption{This table here shows a list of RMSE, MAE(Mean absolute error), and R-squared values. We will be focusing on the The RSME is Root Mean Square Error(RMSE) which is the standard deviation  also known as (prediction errors). We use the RSME since it is used to select the optimal model using the smallest value. The final value used for the model was k = 9 which is the smallest RSME value.}
\end{longtable}

With KNN we did a Cross-Validation for training and testing data. The training set was set to 70 percent then use the model to test our predictions on the test set which is 30 percent. In addition, out full data will include our 3 PCA used to generate 82 percent and "season", "yr", "mnth", "holiday" ,"weekday", "workingday", "weathersit".

\newpage

\begin{figure}
\centering
\includegraphics{RSME_graph.png}
\caption{This figure show the average RMSE of the estimated prediction error on a validation set. The value of K for which the RMSE is lowest is the value of K that we will take. So that is K = 9. We can see as our neighbors increase so does the the error reduce and our neighbors RMSE performance gets better after 10 neighbors with increase in RSME value till 20 neighbors.}
\end{figure}
\newpage

\begin{figure}
\centering
\includegraphics{Predicted_graph.png}
\caption{This figure show there’s a strong correlation between the model’s predictions and its actual results with our K = 9. What K= 9 means it should look into the 9 nearest neighbors and classify the new data point its similar too. In addition, with the $R^2$ = 0.912 being very good since $R^2$ value close to 1 indicated a positive linear association between the predicted vs actual data and out R-squares is 91 percent.}
\end{figure}


\hypertarget{conclusion}{%
\section{10. Conclusion }\label{conclusion}}
In conclusion the least squares can be used in a variety of different methods. From this bike sharing set we are able to showcase how they are used and the different way and formulas to implement least squares. With the implementations of least squares we are ultimately able to find the best molding curve to fit the data based on our bike data set to see the impact our variables make against each other and how bike counts effect environmental issues. 


\hypertarget{reference}{%
\section{11. Reference}\label{refrence}}

\bibitem{Bike Sharing Dataset} 
Hadi Fanaee-T: UCI Machine Learning Repository,
\\\texttt{https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset}

\bibitem{Least Squares, Pseudo-Inverses, PCA & SVD} 
Guido Gerig: Chapter 11 Least Squares, Pseudo-Inverses, PCA & SVD,
\\\texttt{http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf}

\bibitem{k Nearest Neighbor Classifier ( kNN )-Machine Learning Algorithms} 
Shubham Panchal: k Nearest Neighbor Classifier ( kNN )-Machine Learning Algorithms,
\\\texttt{https://medium.com/@equipintelligence/k-nearest-neighbor-classifier-knn-machine\\
-learning-algorithms-ed62feb86582}


\end{document}