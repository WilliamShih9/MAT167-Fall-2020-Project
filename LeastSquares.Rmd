---
title: "LeastSquares"
output:
  pdf_document:
    keep_tex: true
header-includes:
- \usepackage[width=\textwidth]{caption}
- \setcounter{MaxMatrixCols}{20}
---



\newpage

```{r setup, include=FALSE}

#Sys.setenv(PATH = paste("C:/RBuildtools/3.5/bin", Sys.getenv("PATH"), sep=";"))
#Sys.setenv(BINPREF = "C:/RBuildtools/3.5/mingw_$(WIN)/bin/") 
knitr::opts_chunk$set(echo = FALSE)
library(pracma)
library(tidyverse)
library(scales)
library(microbenchmark)
library(Rcpp)
library(RcppArmadillo)



# There were two files from this dataset. There is the 'hour.csv' and 'day.csv'.
# This converts the 'hour.csv' to 'day.csv' file (summarizes the data from each hour to each day)
convert_hour_to_day <- function(hour){
  require(tidyverse)
  day = hour %>%
    group_by(dteday, season, yr, mnth, holiday, weekday, workingday) %>%
    summarize(weathersit = as.integer(round(mean(weathersit))),
              temp = mean(temp),
              atemp = mean(atemp),
              hum = mean(hum),
              windspeed = mean(windspeed),
              casual = sum(casual),
              registered = sum(registered),
              cnt = sum(cnt))
  day = tibble::rowid_to_column(day, "instant")
  return(day)
}

# Helper function for plot_by_hour for quantiles
# https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-summarise/
quibble <- function(x, q = c(0.25, 0.5, 0.75)) {
  tibble(x = quantile(x, q), q = q)
}
 
# Boxplot (lower end is 10th, lower end of box is 25th, middle line is 50th, upper end of box is 75th, upper end is 90th)
# Hour = This is the 'hour.csv' dataset
# Variable = This is the number of users. Either 'registered', 'casual', or 'cnt' (sum of registered or casual).
# Quantiles = Optional. Function needs exactly 5 values 
plot_by_hour <- function(hour, variable, quantiles = c(0.10, 0.25, 0.5, 0.75, 0.90)){
  require(tidyverse)
  require(scales)
  data = hour %>%
    group_by(hr) %>%
    summarise(x = quibble(get(variable), quantiles))
  data = tibble(hr = as.character(data$hr), x = data$x$x, q = data$x$q)
  quant_title = paste(ordinal(quantiles*100), collapse = ", ") 
  quant_title = paste0(quant_title, " percentile ", variable, " of each hour of day")
  plo = ggplot(data, aes(x = reorder(hr, sort(as.numeric(hr))), y = x)) + 
    theme_bw() +
    geom_boxplot(aes(fill = reorder(hr, sort(as.numeric(hr))))) +
    xlab("Hour of Day") +
    ylab(variable) +
    theme(legend.position = "none") +
    ggtitle(quant_title) 
  return(plo)
}

# Day = This is the 'day.csv' dataset
# Variables = These are the variables plotted. The variable plotted should be on the same scale
plot_by_day <- function(day, variables){
  require(tidyverse)
  require(reshape2)
  frame = data.frame(
    day = as.Date(day$dteday)
  )
  for (i in 1:length(variables)){
    frame = cbind(frame, day[variables[i]])
  }
  title = paste(variables, collapse = ", ")
  title = paste0("Date and Value of ", title)
  frame_gg <- melt(frame, id="day")
  plo = ggplot(frame_gg, aes(x = day, y=value, color = variable)) +
    geom_line()+
    xlab("Date") +
    scale_x_date(date_breaks = "3 month", date_labels = "%m-%Y") +
    ylab("Value") +
    theme_bw() +
    ggtitle(title)
  return(plo)
}


# Finds the design matrix. No conversion to dummy variables allowed
# Hour = This is the 'hour.csv' dataset
# x = This is the independent variable(s)
# degrees = The degrees of the polynomials for the independent variable(s): Must be 1 or higher
design_matrix <- function(hour, x, degrees = rep(1, length(x))){
  mat = matrix(, nrow = nrow(hour), ncol = sum(degrees)+1)
  name = c("(Intercept)")
  mat[,1] = 1
  k = 2
  for (i in 1:length(degrees)){
    data = hour[[x[i]]]
    for (j in 1:degrees[i]){
      mat[,k] = data^j
      if (j == 1){
         name = append(name, x[i])
      }
      else{
         name = append(name, paste0(x[i], "^", j))
      }
      k = k + 1
    }
  }
  colnames(mat) = name
  return(mat)
}

# C++ version of the design_matrix function
Rcpp::cppFunction('
NumericMatrix design_matrix_Cpp(List hour, std::vector<std::string> x, IntegerVector degrees){
  int total = 1;
  for (int i = 0; i < degrees.size(); i++){
    total = total + degrees[i];
  }
  CharacterVector name(total);
  name[0] = "(Intercept)";
  NumericVector v = hour[0];
  NumericMatrix mat(v.length(), total);
  std::fill(mat.begin(), mat.begin()+v.length(), 1);
  int k = 1;
  for (int i = 0; i < degrees.size(); i++){
    NumericVector v = hour[x[i]];
    for (int j = 1; j <= degrees[i]; j++){
      mat(_,k) = pow(v, j);
      if (j == 1){
        name[k] = x[i];
      }
      else{
        name[k] = x[i] + "^" + std::to_string(j);
      }
      k++;
    }
    
  }
  colnames(mat) = name;
  return(mat);
}
')

#Finds x for Ax = b using normal equations
normal_equations <- function(A, b){
  x = solve(t(A) %*% A) %*% t(A) %*% b
  return(x)
}

# C++ version for normal_equations
Rcpp::cppFunction('
  arma::vec normal_equations_Cpp(arma::mat A, arma::vec b){
    arma::vec coeffs = inv(trans(A) * A) * trans(A) * b;
    return(coeffs);
  }
', depends='RcppArmadillo')

# C++ version for qr.solve
Rcpp::cppFunction('
  arma::vec qr_solve_Cpp(arma::mat A, arma::vec b){
    arma::mat Q;
    arma::mat R;
    arma::qr_econ(Q, R, A);
    arma::vec x = inv(R)*trans(Q)*b;
    return(x);
  }
', depends = 'RcppArmadillo')

# Finds x for Ax=b using SVD
svd_solve <- function(A, b){
  x = svd(A)
  x = as.numeric(x$v %*% (t(x$u) %*% b / x$d))
  names(x) = colnames(A)
  return(x)
}

# C++ version of svd_solve
Rcpp::cppFunction('
  arma::vec svd_solve_Cpp(arma::mat A, arma::vec b){
    arma::mat U;
    arma::vec s;
    arma::mat V;
    arma::svd_econ(U, s, V, A);
    arma::vec x = V * (trans(U)*b/s);
    return(x);
  }
', depends = 'RcppArmadillo')



# Convert condition numbers to LaTeX output
# Digits can be from 0 to 7 inclusive
scientific_to_latex <- function(x, digits = 3){
  x = scientific(as.numeric(x), digits = digits)
  x = paste0("$", substr(x, 1, 1+digits), " \\times 10^{", as.numeric(substr(x, stringr::str_length(x)-2, stringr::str_length(x))), "}$")
  return(x)
}

# Used for curve fitting
# Coefficients = coeffieints for the polynomial term: The first value is power of one, second value is quadratic, third value is cubic, etc..
# Constant = constant term added to each polynomial term
# Range = x values
polynomial <- function(coefficients, constant = 0, range = seq(0, 23, 0.1)){
  A = vector(mode = "numeric", length = length(range))
  for (i in c(1:length(range))){
    A[i] = constant
    for (j in c(1:length(coefficients))){
      A[i] = A[i] + coefficients[j]*range[i]^j
    }
  }
  return(data.frame(range, A))
}

# End List of Functions

# 1. Introduction
```



# 1. Introduction

The 
\color{blue}
\href{https://www.archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset}{dataset}
\color{black}
for this project is the "Bike Sharing Dataset Data Set" found in the UCI Machine Learning Repository. The dataset contains hourly count of rental bikes for all of 2011 and 2012 (January 1, 2011 to December 31, 2012) in the Capital Bikeshare System of Washington D.C. area (Washington-Arlington-Alexandria, DC-VA-MD-WV metropolitan area). The UCI Machine Learning Repository cites Hadi Fanaee-T from the "Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto" for the compilation of the data.

The dataset is outdated since data is actually available up to November 2020 on Capital Bikeshare's website (as of December 18, 2020), but this limited dataset will still work for the purposes of demonstrating linear algebra on a real world dataset.

There are two files included in the dataset: a `hour.csv` and a `day.csv`. We will use the `hour.csv` for the regression, since the `day.csv` is simply just a sumamry of the `hour.csv` file. We also made a function that easily converts the `hour.csv` to the `day.csv` called `convert_hour_to_day()`.

There are 14 different variables that are in this dataset that are potentially of interest. Two variables are not useful and immediately thrown out: `instant` (this is simply the row number of the dataset) and `dteday` (date of the year).

Denote $x_{n}$ as plausible independent variables and denote $y_{n}$ as plausible dependent variables.

$x_{1}$: `season` (1: spring, 2: summer, 3: fall, 4: winter)

$x_{2}$: `yr` (0: 2011, 1: 2012)

$x_{3}$: `mnth` (1 to 12)

$x_{4}$: `hour` (0 to 23)

$x_{5}$: `holiday` (whether a holiday (0 or 1) from
\color{blue}
\href{https://dchrc.dc.gov/page/holiday-schedule}{this list of holidays}
\color{black}
)

$x_{6}$: `weekday` (0 to 6)

$x_{7}$: `workingday` (1 if weekday and not holiday, 0 otherwise)

$x_{8}$: `weathersit`:  Weather conditions (1: Clear, Few clouds, Partly cloudy, Partly cloudy,
 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist,
 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds, 
 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog)

$x_{9}$: `temp` (0-1, normalized temperature in Celsius. Divided by 41)

$x_{10}$: `atemp` (0-1, normalized "feels like" temperature in Celsius. Divided by 50)

$x_{11}$: `hum` (percent humidity)

$x_{12}$: `windspeed` (0-1, Normalized wind speed. Divided by 67)

$y_{1}$: `casual` (count of casual users)

$y_{2}$: `registered` (count of registered users)

$y_{3}$: `cnt` (count of sum of casual and registered users)

The following least squares regression exercise will try to predict the `casual`, `registered`, or `cnt` as a function of the independent variables. Also, we will try some principal components analysis (PCA) and k-nearest neighbors (kNN) with these variables.

## Data Analysis

Preliminary data analysis shows that the `hour` is by far the most important independent variable for explaining the variation in the dependent variables. Thus, it is important to know how exactly the `hour` variable interacts with `registered`, `casual`, and `cnt`. 

We also found that it makes sense to treat `hour` as a categorical variable (treat `hour` as 23 independent dummy variables (0 or 1 for each variable), one for each hour minus the constant term), but in this case, we will try to fit `hour` in terms of a polynomial curve to demonstrate polynomial fitting with linear algebra.

A plot of `hour` on the x-axis and `registered` or `casual` on the y-axis would us some insight of what degree polynomial for `hour` we should be looking for.

```{r, echo = FALSE, message = FALSE,  fig.pos = "!h", fig.cap = "Boxplot of registered users for Jan 1, 2011 to December 31, 2011 for each hour of day. The low whisker represents 10th percentile, the box represents the interquartile range, the top whisker represents the 90th percentile. For example, with 731 days in the datset, the low whisker represents the 73rd lowest value.",  fig.width = 8, fig.height = 4}
dataset = read.csv("hour.csv")

#Figure 1
plot_by_hour(dataset, "registered")
```

**Figure 1** for the number of registered users show a trimodal distribution with three peaks throughout the day. A possible interpretation of the three peaks is that there is an early peak for the morning commute, a central peak for lunchtime, and a late peak for the evening commute. This suggests that we need a high-degree polynomial to accurately the number of registered users throughout the day. A six-degree polynomial is the minimum degree that can represent a trimodal distribution.

```{r, echo = FALSE, message = FALSE,  fig.pos = "!h", fig.cap = "Boxplot of casual users for Jan 1, 2011 to December 31, 2011 for each hour of day. The low whisker represents 10th percentile, the box represents the interquartile range, the top whisker represents the 90th percentile. For example, with 731 days in the datset, the low whisker represents the 73rd lowest value.",  fig.width = 8, fig.height = 4}

# Figure 2
plot_by_hour(dataset, "casual")
```


**Figure 2** for the number of casual users show a single peak distribution. A possible reason for this is that casual users are tourists that don't commute. Tourists also don't wake up early in the morning and most things to do for tourists occur in the afternoon and evening. Thus, the peak occurs at around 12 PM-6 PM. A two-degree polynomial is potentially sufficient to represent the number of casual users throughout the day.

**Figure 1** and **Figure 2** show significant variation so that it is clear that the hour of the day is not the only variable influencing how many users there are for this bike sharing system. We can plot at the number of users for each day for further information. 

```{r, echo = FALSE, message = FALSE,  fig.pos = "!h",  fig.cap = "Number of casual, registered, and sum of casual and registered for all 731 days in the dataset", fig.width = 8, fig.height = 4}
day = convert_hour_to_day(dataset)

# Figure 3
plot_by_day(day, c("casual","registered","cnt"))
```

**Figure 3** shows both variation through the year and an increase in number of users from 2011 to 2012. This makes sense if this Capital Bike-Sharing was still a developing system in 2011 and not yet a mature system where the market is already saturated. Thus, we want to include a variable in our least squares regression model that includes controls for seasonal variation and the year. This would be `season` and `yr` from the list of $x_{n}$. It turns out that the `weekday` (day of the week) and `mnth` (month of the year) do not explain a large additional amount of variation, so they won't be included in the model.

But **Figure 3** still shows quite a bit of variation day to day within each season. There are quite a few weather related variables in the list of independent variables in the dataset, which would account for some of the remaining variation. 

\newpage

```{r, echo = FALSE, message = FALSE, fig.pos = "!h", fig.cap = 'Normalized temperature and "feels like" temperature for all 731 days in the datset',  fig.width = 8, fig.height = 4}

# Figure 4
plot_by_day(day, c("temp","atemp"))
```

**Figure 4** shows the average `temp` (actual temperature) and `atemp` ("feels like" temperature) for each day of the year. A comparison of **Figure 3** and **Figure 4** shows that temperature shows a strong negative relationship with the number of bike-sharing users, which makes sense. People do not want to bike when it is cold outside.
These variables have a very high correlation with each other such that we found it to be sufficient to just add `temp`. In fact, `temp` is good enough to explain most of the weather-related variation and other variables such as `wind` (wind speed) and `hum` (humidity) are not necessary.


Thus, we choose just `season`, `yr`, `hr`, and `temp` as the independent variables and omit the rest of the variables for the least squares regression model. These are $x_{1}, x_{2}, x_{4}$, and $x_{9}$.

\newpage

# 2. Design Matrix

The design matrix $A$ is the matrix that will be used to solve the equation $Ax = b$, where $x$ are the coefficients, often referred to as the $\beta$'s and $b$ is the dependent variable. It is the matrix of the explanatory/independent variables.

The design matrix $A$ will be of $\mathbb{R}^{17379 \times (n+4)}$, (where $n$ is the maximum degree of the polynomial for $x_{4}$ (`hour`)) since there are 17,379 rows (one for each hour in the dataset) and there are four variables other than the `hour` ($x_{4}$) variable. The other four variables are the constant term, $x_{1}$ (`season`), $x_{2}$ (`yr`), and $x_{9}$ (`temp`).

The following will be the form of our design matrix $A \in \mathbb{R}^{17379 \times (n+4)}$ (Note that $m$ is left the matrix for simplification, but the of $m = 17379$):

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & \dots & x_{14}^{n} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & \dots & x_{24}^{n} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & \dots & x_{34}^{n} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & \dots & x_{m4}^{n} & x_{m9} \\
  \end{bmatrix} 
\]

The first value of the index represents the row number and the second value of the index represents the variable number in the matrix. For example $x_{32}$ in the above means the 3rd row of variable $x_{2}$.

Now the question remaining is the appropriate value of $n$. We know that the answer depends on which dependent variable we are using. If $b$ is `registered` or `cnt`, the value of $n$ should be higher than if $b$ is `casual`. One way to find a good for $n$ is calculate the R-squared for each value of $n$. The R-squared is the proportion of variation in the dependent variable that can be explained by the independent variables. A value of $n$ where $n+1$ does not increase the R-squared significantly further would be a good value of $n$.


```{r, message = FALSE, echo = FALSE}

#2. Design Matrix 

# This is calculating R-squared of regression models 

Rsq_cnt = sapply(1:15, function(x) {
  summary(lm(dataset$cnt ~ design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1)))))$r.squared
})

Rsq_casual = sapply(1:15, function(x) {
  summary(lm(dataset$casual ~ design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1)))))$r.squared
})

Rsq_registered = sapply(1:15, function(x) {
  summary(lm(dataset$registered ~ design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1)))))$r.squared
})

condition = sapply(1:15, function(x) {
  cond(design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1))))
})

frame = cbind(Rsq_cnt, Rsq_casual, Rsq_registered)

hs = c("$x_{4}$",
  "$x_{4}+x_{4}^{2}$",
  "$x_{4}+x_{4}^{2}+x_{4}^{3}$",
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{4}$",
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{5}$",
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{6}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{7}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{8}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{9}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{10}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{11}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{12}$", 
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{13}$",  
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{14}$",
  "$x_{4}+x_{4}^{2}+\\dots+x_{4}^{15}$"
)
rownames(frame) = hs

frame[,1:3] = round(frame[,1:3], digits = 3)


colnames(frame) = c("$R^{2}_{cnt}$", "$R^{2}_{casual}$", "$R^{2}_{registered}$")

# Table 1
knitr::kable(frame, digits = 3, caption = "R-squared values for linear regression of each of value of $n$, maximum power of polynomial for `hour`, including $x_{1}, $x_{2}$, and $x_{9}$", escape = FALSE)
```

**Table 1** shows the values of R-squared regressed upon the dependent variables `cnt`, `casual`, and `registered` for each value of $n$ for values of 1 to 15. For `cnt` and `registered`, a good value of $n$ appears to be 7. $n = 7$ gives a huge increase in R-squared over $n = 6$ for `cnt` and `registered`. Only small increases of R-squared are seen for values of $n$ above 7 for `cnt` and `registered`. Although these increases are still statistically significant, it is not a good idea to have very high order polynomials as such a regression is likely to overfit the data. For example, very high order polynomials may give poor predictions for data in 2013 or beyond. For `casual`, a good value of $n$ appears to be 3. Only small increases of R-squared are seen for values of $n$ above 3 for `casual`.

Therefore, the design matrix if $b$ is `cnt` or `registered` is $A \in \mathbb{R}^{17379 \times 11}$, where $m = 17379$:

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & x_{14}^{3} & x_{14}^{4} & x_{14}^{5} & x_{14}^{6} & x_{14}^{7} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & x_{24}^{3} & x_{24}^{4} & x_{24}^{5} & x_{24}^{6} & x_{24}^{7} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & x_{34}^{3} & x_{34}^{4} & x_{34}^{5} & x_{34}^{6} & x_{34}^{7} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & x_{m4}^{3} & x_{m4}^{4} & x_{m4}^{5} & x_{m4}^{6} & x_{m4}^{7} & x_{m9} \\
  \end{bmatrix} 
\]

The design matrix if $b$ is `casual` is $A \in \mathbb{R}^{17379 \times 7}$, where $m = 17379$:

\[
A=
  \begin{bmatrix}
    1 & x_{11} & x_{12} & x_{14} & x_{14}^{2} & x_{14}^{3} & x_{19} \\
    1 & x_{21} & x_{22} & x_{24} & x_{24}^{2} & x_{24}^{3} & x_{29} \\
    1 & x_{31} & x_{32} & x_{34} & x_{34}^{2} & x_{34}^{3} & x_{39} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots  \\
    1 & x_{m1} & x_{m2} & x_{m4} & x_{m4}^{2} & x_{m4}^{3} & x_{m9} \\
  \end{bmatrix} 
\]



```{r, message = FALSE, echo = FALSE}

# Comparing speed of custom design_matrix function versus built in mode.matrix.lm function
Compare = microbenchmark("design_matrix()" = {S = design_matrix(dataset, c("yr","season","hr","temp"), c(1,1,7,1))},
                       "design_matrix_Cpp()" = {S = design_matrix_Cpp(dataset, c("yr","season","hr","temp"),
                                                                  as.integer(c(1,1,7,1)))},
                       "model.matrix.lm()" = 
                         {S = model.matrix(~yr+mnth+hr+I(hr^2)+I(hr^3)+I(hr^4)+I(hr^5)+I(hr^6)+I(hr^7)+temp,
                                                          data = dataset)},
                       times = 100)
# Table 2
knitr::kable(summary(Compare), digits = 3, caption = "Comparison of speed (in milliseconds) of custom design_matrix() functions with built-in R model.matrix.lm() function", escape = FALSE)

```

We made a custom function called `design_matrix()` in R and a Rcpp (C++) version called `design_matrix_Cpp()` to form the design matrix. 
\color{blue}
\href{https://teuder.github.io/rcpp4everyone_en/index.html}{This Rcpp tutorial}
\color{black}
was useful in converting the R functions to C++. The R implementation was slightly faster than the base built-in R version called `model.matrix.lm()` and the C++ version was three times faster than built-in R version. **Table 2** shows a speed comparison using R package `microbenchmark` showing minimum, 25th percentile, 50th percentile, mean, 75th percentile, and max time of 100 replications of the function to form the design matrix with $n$ = 7 and $A \in \mathbb{R}^{17379 \times 11}$.



# 3. Normal Equation

The simplest way to solve $Ax = b$ would be to do use the normal equations.

The solution to $Ax = b$ using the normal equations is $x = (A^{T}A)^{-1}A^{T}b$

However, computers cannot represent real numbers exactly. The condition number $\kappa(A)$ of the input matrix $A$ represents how much error there could be in the output. The condition number of $A^{T}A$ is equal to condition number of $A^{2}$. For our example, as $n$ increases (a more complex and higher order polynomial), the condition number $\kappa(A)$ increases significantly. If the condition number is too high, then a computer will treat a non-singular matrix as singular. Then, there will be very large errors in the computation.

```{r, message = FALSE, warning = FALSE, echo = FALSE}

#3. Normal Equations
A = design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,3,1)))
b = dataset$casual

result = sapply(1:15, function(x)  {
  A = design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1)))
  x = tryCatch(expr = 
                 {x = solve(t(A) %*% A) %*% t(A) %*% b
                  x = max((svd_solve(A, b) - x)/svd_solve(A,b))},
               error = function(e){
                 str = paste0('Error, Recripocal $\\kappa(A)$: ', substr(e[[1]], 67, 999))
                 return(str)
               })
  return(x)
})

result2 = sapply(1:15, function(x)  {
  A = design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,x,1)))
  x = tryCatch(expr = 
                 {x = normal_equations_Cpp(A, b)
                  x = max((svd_solve(A, b) - x)/svd_solve(A,b))},
               error = function(e){
                 str = paste0('Error, Recripocal $\\kappa(A)$: ', substr(e[[1]], 67, 999))
                 return(str)
               })
  return(x)
})

column1 = scientific_to_latex(condition, 3)
column2 = scientific_to_latex(condition^2, 3)
column3 = scientific_to_latex(result[1:5], 3)
column3b = scientific_to_latex(substr(result[6:15], 32, 999), 3)
column3c = paste0(substr(result[6:15], 1, 31), column3b)
column3d = c(column3, column3c)
column4 = scientific_to_latex(result2, 3)
frame = cbind(column2, column3d, column4)

colnames(frame) = c("$\\kappa(A)^{2}$", "Relative error/error message R", "Relative error C++")

# Table 3
rownames(frame) = hs

knitr::kable(frame, escape = FALSE, caption = "Condition number of each value of $n$ (maximum power of polynomial for `hour`) and relative error of normal equations versus SVD")
```

**Table 3** shows the square of the condition number for matrix $A$, the relative error/error message when solving $Ax = b$ for each value of $n$ using R, and the relative error using Rcpp (C++). The relative error is calculated as the maximum error of any coefficient of the normal equations when compared to SVD (SVD is considered computationally precise). 

However, R gives an error trying to find the inverse if the matrix is close to singular. The error message is the following: `Error in solve.default(t(A) %*% A): system is computationally singular: reciprocal condition number ='. The reciprocal condition number given in the error message by R is close to the conditional number of $A^{2}$. On the other hand, RcppArmadillo does not give an error when finding the inverse of a system with a very high condition number. When $n > 9$ and $\kappa(A^{T}A) > 10^{29}$, the relative error is 100% or higher. R appears to give an error if the relative error is greater than 0.1% and $\kappa(A^{T}A) > 10^{17}$. With such large errors possible, it is not recommended to ever use the normal equations when solving $Ax = b$ on a computer.

```{r, message = FALSE, echo = FALSE}
Compare2 = microbenchmark("normal_equations(A, b)" = {S = normal_equations(A, b)},
                       "normal_equations_Cpp(A, b)" = {S = normal_equations_Cpp(A, b)},
                        times = 100)
# Table 4
knitr::kable(summary(Compare2), digits = 3, caption = "Comparison of speed (in milliseconds) of solving $Ax = b$ where $A \\in \\mathbb{R}^{17379 \\times 7}$ using normal equations implemented in R vs Rcpp (C++)")
```

**Table 4** shows that the Rcpp (C++) implementation is on average 4 times faster than the R implementation of solving the normal equations.

# 4. QR Decomposition

Another way, a more computationally accurate way, to solve $Ax = b$ to use QR decomposition.

First, find the reduced QR decomposition such that $A = \hat{Q}\hat{R}$, $\hat{Q}$ is an orthogonal matrix, and $\hat{R}$ is an upper triangular matrix. $\hat{Q}$ meets the following condition: $\hat{Q}^{T}\hat{Q} = \hat{Q}\hat{Q}^{T} = I$. Also, $\hat{Q} \in \mathbb{R}^{17379 \times 7}$ and $\hat{R} \in \mathbb{R}^{7 \times 7}$ in this case as $A \in \mathbb{R}^{17379 \times 7}$

The solution to $Ax = b$ using QR decomposition is $x = \hat{R}^{-1}\hat{Q}^{T}b$.

```{r, message = FALSE, echo = FALSE, message = FALSE}
#QR decomposition

Compare3 = microbenchmark("qr.solve(A, b)" = {S = qr.solve(A, b)},
                       "qr_solve_Cpp(A, b)" = {S = qr_solve_Cpp(A, b)},
                        times = 100)
# Table 5
knitr::kable(summary(Compare3), digits = 3, caption = "Comparison of speed (in milliseconds) of solving $Ax = b$ where $A \\in \\mathbb{R}^{17379 \\times 7}$ using QR decomposition implemented in R vs Rcpp (C++)")

```

**Table 5** shows that the Rcpp (C++) implementation is slightly faster than R implementation of using QR decomposition to solve $Ax = b$.

# 5. Singular Value Decomposition

A longer way to solve $Ax = b$, but an even more accurate way than QR decomposition is to use Singular Value Decomposition.

First, find the reduced SVD such that $A = \hat{U}\hat{\Sigma}V^{T}$.

Also, $\hat{U} \in \mathbb{R}^{17379 \times 7}$, $V \in \mathbb{R}^{7 \times 7}$, and $\hat{\Sigma} \in \mathbb{R}^{7 \times 7}$ in this case as $A \in \mathbb{R}^{17379 \times 7}$.

The solution to $Ax = b$ using SVD is $x = V(\hat{U}^{T}b/\hat{\Sigma})$

```{r, message = FALSE, echo = FALSE}
#5. Singular Value Decomposition
Compare4 = microbenchmark("svd_solve(A, b)" = {S = svd_solve(A, b)},
                       "svd_solve_Cpp(A, b)" = {S = svd_solve_Cpp(A, b)},
                        times = 100)
# Table 6
knitr::kable(summary(Compare4), digits = 3, caption = "Comparison of speed (in milliseconds) of solving $Ax = b$ where $A \\in \\mathbb{R}^{17379 \\times 7}$ using SVD implemented in R vs Rcpp (C++)")

```

**Table 6** shows that the Rcpp (C++) implementation is slightly faster than R implementation of using SVD to solve $Ax = b$.

Now, it is time to compare the speed of solving $Ax = b$. If theory is correct, the normal equations should be fastest, followed by QR decomposition, and followed by SVD. 


```{r, message = FALSE, echo = FALSE}
A = design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,7,1)))

Result2 = microbenchmark("normal_equations_Cpp(A, b)" = {S = normal_equations_Cpp(A, dataset$cnt)},
                        "qr.solve(A, b)" = {S = qr.solve(A, dataset$cnt)},
                       "qr_solve_Cpp(A, b)" = {S = qr_solve_Cpp(A, b)},
                       "svd_solve(A, b)" = {S = svd_solve(A, dataset$cnt)},
                       "svd_solve_Cpp(A,b)" = {S = svd_solve_Cpp(A, b)},
                       times = 100)
# Table 7
knitr::kable(summary(Result2), digits = 3, caption = "Comparison of speed (in milliseconds) of solving $Ax = b$ where $A \\in \\mathbb{R}^{17379 \\times 11}$ using QR decomposition or SVD implemented in R vs Rcpp (C++)")

```

Note that **Table 7** uses $n=7$, the design matrix for `registered` and `cnt` instead of $n=3$, the design matrix for `casual` in **Table 4, 5, 6**.

Normal equations using Rcpp (C++) is about 3 times faster than QR decomposition using C++. QR decomposition is about 70% faster than SVD. QR decomposition implemented in R is about twice as fast as using SVD implemented in R. Thus, we see that the results from **Table 7** are in line with what we should expect from theory.

# 6. Final Regression Model

Recall that $y_{1}$ = `casual`, $y_{3}$ = `cnt`, $x_{1}$ = `season`, $x_{2}$ = `yr`, $x_{4}$ = `hour`, $x_{9}$ = `temp` for below.

$y_{1} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{4} + \beta_{4}x_{4}^{2} + \beta_{5}x_{4}^{3} + \beta_{6}x_{9}$

$y_{3} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{4} + \beta_{4}x_{4}^{2} + \beta_{5}x_{4}^{3} + \beta_{6}x_{4}^{4} + \beta_{7}x_{4}^{5} + \beta_{8}x_{4}^{6} + \beta_{9}x_{4}^{7} + \beta_{10}x_{9}$

The solution to $Ax = b$ for $y_{1}$ is:


\[
x =
  \begin{bmatrix}
    \beta_{0} \\
     \beta_{1} \\
    \beta_{2} \\
     \beta_{3}\\
     \beta_{4} \\
    \beta_{5} \\
    \beta_{6} \\
  \end{bmatrix} =
  \begin{bmatrix}
    -42.145 \\
    0.6875 \\
    12.887 \\
    -4.859 \\
    1.275 \\
    -0.047 \\
    89.528 \\
  \end{bmatrix}
\]

The solution to $Ax = b$ for $y_{3}$ is:

\[
x =
  \begin{bmatrix}
    \beta_{0} \\
     \beta_{1} \\
    \beta_{2} \\
     \beta_{3}\\
     \beta_{4} \\
    \beta_{5} \\
    \beta_{6} \\
    \beta_{7}\\
     \beta_{8} \\
    \beta_{9} \\
    \beta_{10} \\
  \end{bmatrix} =
  \begin{bmatrix}
    -145.05 \\
    17.18 \\
    88.57 \\
    29.45 \\
    -63.199 \\
    24.27 \\
    -3.62 \\
    0.258 \\
    -0.0088 \\
    0.000116 \\
    243.131 \\
  \end{bmatrix} 
\]

Inputting the solution results in:

$y_{1} = -42.145 + 0.6875x_{1} + 12.887x_{2} - 4.859x_{4} + 1.275x_{4}^{2} - 0.047x_{4}^{3} + 89.528x_{9}$

$y_{3} = -145.05 + 17.18x_{1} + 88.57x_{2} + 29.45x_{4} - 63.199x_{4}^{2} + 24.27x_{4}^{3} - 3.62x_{4}^{4} + 0.258x_{4}^{5} - 0.0088x_{4}^{6} + 0.000116x_{4}^{7} + 243.131x_{9}$

Note that $\frac{1}{n}\sum_{i=1}^{n} x_{1} = \bar{x_{1}} = 2.50164, \frac{1}{n}\sum_{i=1}^{n} x_{2} = \bar{x_{2}} = 0.5025606, \frac{1}{n}\sum_{i=1}^{n} x_{8} = \bar{x_{8}} = 0.4970$.

Input the means of $x_{1}, x_{2}$, and $x_{8}$ into the formulas of $y_{1}$ and $y_{3}$.

This results in eliminating all independent variables that aren't `hour` ($x_{4}$), resulting in the following two functions:

$y_{1} = -4.859x_{4} + 1.275x_{4}^{2} -0.047x_{4}^{3} + 10.54523$

$y_{3} = 29.45x_{4} - 63.199x_{4}^{2} + 24.27x_{4}^{3} - 3.62x_{4}^{4} + 0.258x_{4}^{5} - 0.0088x_{4}^{6} + 0.000116x_{4}^{7} + 63.26103$.

This now allows us to graph `casual` ($y_{1}$) and `registered` ($y_{3}$) as a function of $x_{4}$ (`hour`) and see how well the least squares regression fits the actual data.

\newpage

```{r, echo = FALSE, message = FALSE, fig.cap = "The 25th, 50th, and 75th percentile of casual users (731 days) are graphed for each hour of the day. The mean values of season, yr, and temp are entered into the regression solution to give a constant value. Thus, the regression is now only in terms of hour. Then, the mean curve fit of the regression can be compared to the 25th, 50th, and 75th percentiles of casual users.", fig.width = 8, fig.height = 4}
# 6. Final Regression Model
y1 = svd_solve(design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,3,1))), dataset$casual)
y3 = svd_solve(design_matrix_Cpp(dataset, c("yr","season","hr","temp"), as.integer(c(1,1,7,1))), dataset$cnt)

constant_y1 = y1[1] + mean(dataset$yr)*y1[2] + mean(dataset$season)*y1[3] + mean(dataset$temp)*y1[7]

constant_y3 = y3[1] + mean(dataset$yr)*y3[2] + mean(dataset$season)*y3[3] +
mean(dataset$temp)*y3[11]

coeff_y1 = y1[4:6]

coeff_y3 = y3[4:10]

predicted_casual = polynomial(coeff_y1, constant_y1)
predicted_cnt = polynomial(coeff_y3, constant_y3)
predicted_casual = cbind(predicted_casual, "Curve Fit")
predicted_cnt = cbind(predicted_cnt, "Curve Fit")
colnames(predicted_casual) = c("hr","x", "q")
colnames(predicted_cnt) = c("hr", "x", "q")
real_casual = dataset %>%
  group_by(hr) %>%
  summarise(x = quibble(casual, c(0.25,0.5,0.75)))
real_cnt = dataset %>%
  group_by(hr) %>%
  summarise(x = quibble(cnt, c(0.25,0.5,0.75))) 

real_casual$q = paste0(sprintf("%.f", real_casual$x$q*100), "th")
real_cnt$q = paste0(sprintf("%.f", real_cnt$x$q*100), "th")
real_casual$x = real_casual$x$x
real_cnt$x = real_cnt$x$x

casual = rbind(predicted_casual, real_casual)
cnt = rbind(predicted_cnt, real_cnt)

#Figure 5
ggplot(casual, aes(x = hr, y = x, group = q, color = q)) +
  geom_line() +
  xlab("Hour") +
  ylab("Number of casual users") +
  ggtitle("25th, 50th, 75th percentile of casual users compared to mean curve fit (as function of hour)") +
  scale_x_continuous(limits = c(0, 24), breaks = seq(0, 24, 2)) +
  theme_bw()
```

**Figure 5** shows that the cubic polynomial function is not a great fit for hours 4 to 6 as the mean curve fit is above the 75th percentile. The curve does not dip down far enough from hours 4 to 6. Note that interquartile range (75th percentile minus 25th percentile) for `casual` is very large from **Figure 5** which might make predicting data quite difficult. Also **Figure 5** suggests that `casual` is skewed right since the mean curve fit is almost always above the 50th percentile.

\newpage

```{r, echo = FALSE, message = FALSE, fig.cap = "Same description as for Figure 5, except the variable is now cnt (sum of registered and casual users) rather than casual.", fig.width = 8, fig.height = 4}
#Figure 6
ggplot(cnt, aes(x = hr, y = x, group = q, color = q)) +
  geom_line() +
  xlab("Hour") +
  ylab("Number of casual+registered users") +
  scale_x_continuous(limits = c(0, 24), breaks = seq(0, 24, 2)) +
  ggtitle("25th, 50th, 75th percentile of cnt users compared to mean curve fit (as function of hour)") +
  theme_bw()
```

**Figure 6** shows the 7-degree (septic) polynomial function is quite a good fit for `cnt`. However, near hour 22, the mean curve fit starts to slope upwards again, which is not correct. Also, similar to **Figure 5**, the mean curve fit overestimates the number of users from hours 4 to 6 again.

